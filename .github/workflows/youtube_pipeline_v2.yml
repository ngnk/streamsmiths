name: YouTube Analytics Pipeline V2

on:
  schedule:
    # Run every hour at minute 0
    - cron: '0 */8 * * *'
  
  # Allow manual trigger from GitHub UI
  workflow_dispatch:

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run Bronze Layer (YouTube API Ingestion)
        env:
          YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}
          YOUTUBE_CHANNEL_IDS: ${{ secrets.YOUTUBE_CHANNEL_IDS }}
        run: |
          echo "ðŸ”µ Starting Bronze Layer - Fetching data from YouTube API..."
          python bronze_v2.py
      
      - name: Run Silver Layer (Data Transformation)
        run: |
          echo "âšª Starting Silver Layer - Transforming to Parquet..."
          python silver_v2.py
      
      - name: Run Gold Layer (Load to Neon)
        env:
          NEON_DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}
        run: |
          echo "ðŸŸ¡ Starting Gold Layer - Loading to Neon database..."
          python gold_v2.py
      
      - name: Pipeline Complete
        run: |
          echo "âœ… Pipeline V2 completed successfully!"
          echo "Data loaded to: channels_log_v2, videos_log_v2, trending_videos_log_v2"
      
      # Optional: Upload artifacts for debugging
      - name: Upload Bronze Data (if failed)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: bronze-data-debug
          path: bronze_data_v2/
          retention-days: 7
      
      - name: Upload Silver Data (if failed)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: silver-data-debug
          path: silver_data_v2/
          retention-days: 7
